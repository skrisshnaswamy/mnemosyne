So the term ~={pink}**auto-regressive**=~ simply means that the output of a model is sequential (one token at a time). The way I see it, it is technically a property of the outcome or rather the use case. But then again, in order to generate the outcome sequentially, the models need to be architected in a way that it can support this. Such models are typically called **Auto regressive models**.

Model architectures based on the transformer arch. **and** which has a decoder network typically are capable of **auto regression** output generation. But that said, there are some subtleties.
Decoder-only models are by design auto-regressive - They are typically designed for solving use cases like next token prediction / generation (Sentence completion task). And they do this by a technique called [[Masking]]. You see they **mask** (hide) the future tokens during training to mimic that when the model is training, it actually cannot see what comes next, but only see the past tokens. This is also an [[Attention]] mechanism and is called the [[Causal Attention]].