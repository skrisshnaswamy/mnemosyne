Its a technique to distill knowledge from 1 model to another - typically from a _big_ **teacher** model to a _small_ **student** model; and in rare cases to itself (via techniues like rejection sampling (best of N) - where you sample outcomes from the previous training loop - ~={red} Need to learn more on how this is done=~)

### Papers to refer
- Seminal work on [**Knowledge distillation** by Hinton 2015](https://arxiv.org/pdf/1503.02531)
- 