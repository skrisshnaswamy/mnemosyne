It is a way to identify the importance of stuff.
It gets used in computer vision to identify what parts of the image are of high importance.
It is also used in Explainable AI, to show what parts of the inputs did the model give more importance / attention to.
We use something called [[Saliency#Saliency Maps|saliency maps]] to visualize the model's thinking process

---
# Saliency Maps
It's a way to visualize the importance. Used in Explainable AI